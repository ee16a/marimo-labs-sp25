{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Recognition Lab 1: SVD/PCA Classification\n",
    "\n",
    "### EECS 16A: Designing Information Devices and Systems I, Fall 2024\n",
    "\n",
    "\n",
    "Junha Kim, Jessica Fan, Savit Bhat, Jack Kang (Fall 2024).\n",
    "\n",
    "\n",
    "This lab was heavily inspired by previous EECS16B lab 8, written by Nathaniel Mailoa, Emily Naviasky, et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lab 1: SVD/PCA\n",
    "\n",
    "* [Task 1: Data Collection](#part1)\n",
    "* [Task 2: Data Preprocessing](#part2)\n",
    "* [Task 3: PCA via SVD](#task3)\n",
    "* [Task 4: Clustering Data Points](#task4)\n",
    "* [Task 5: Testing your Classifier](#task5)\n",
    "* [Task 6: Live Testing](#task6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## <span style=\"color:navy\">Introduction</span>\n",
    "\n",
    "Throughout lectures and discussion, you have learned theoretical concepts of the DFT and SVD. In the voice recognition lab module, we'll be applying these concepts from class to build an audio classifier using Mel-Scaled Short-Time Fourier transform (STFT) and Principal Component Analysis (PCA), an application of the Singular Value Decomposition (SVD).\n",
    "\n",
    "This lab takes in voice recordings, sampling the waveform of an analog signal into discrete points. Multiple syllables will generate multiple peaks and troughs, and soft syllables will have discrete points of lower magnitude than hard syllables (eg. words like \"here\" and \"pear\" will have very similar voice recordings)\n",
    "\n",
    "In this module, you will build a voice classifier for words of different syllables and intonation in two parts:\n",
    "- In lab 1, we will explore dimensionality reduction and classification of voice signals using PCA.\n",
    "- In lab 2, we will extend our previous scheme by preprocessing our audio signal with the STFT (very similar to spectrograms that you have seen in the Shazam lab) and feed that as an input to our previous PCA classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Classification Procedure\n",
    "Below is an overview of the classification procedure we will be following in this lab.\n",
    "1. Collect recordings of different words. This will form our data set.\n",
    "2. Split the data into 2 sets: training and testing data\n",
    "3. Preprocess the data to align the words.\n",
    "4. Perform SVD and evaluate on the training data split.\n",
    "5. Classify each word using clustering in the PCA basis.\n",
    "6. Evaluate performance of your PCA model by running the model on testing data.\n",
    "7. Make sure you (and your GSI) are satisfied with the classifier's accuracy.\n",
    "\n",
    "### Side Note: Datasets in Machine Learning Applications\n",
    "It is common practice in machine learning applications to split a dataset into a training set and testing set (some common ratios for train:test are 80:20 or 70:30). In this lab, we will split 70:30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sounddevice\n",
    "import pyaudio\n",
    "from tqdm.notebook import trange\n",
    "from IPython import display\n",
    "import wave\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import utils\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "cm = ['blue', 'red', 'green', 'orange', 'black', 'purple']\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='part1'></a>\n",
    "## <span style=\"color:navy\">Task 1: Data Collection</span>\n",
    "\n",
    "We will begin by collecting our data. We will provide data for 4 words for the sake of time, so we ask that you record a set of 40 recordings for one word of your choosing.\n",
    "\n",
    "When humans distinguish words, they listen for temporal and frequency differences to determine what is being said. Let's keep that in mind when we choose our words, as we will eventually be performing a frequency transform pre-classification in the Mel-Scaled STFT.\n",
    "\n",
    "When you think of speech signals, you might notice that each word/sound has a disctinctive shape. Taking the shape of the magnitude of a signal is called enveloping, exemplified in the plot below. We want to do some filtering to retrieve the envelope of the audio signal, which we will use for calculating the SVD.\n",
    "\n",
    "**Given this information, brainstorm one word for your recordings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voice Recordings\n",
    "\n",
    "Now we will record **40** audio samples for one word. We recommend that you split the recording between partners to maximize variance between the recorded signals. For each word, make sure to note who said it and how it was said (like through a video/audio recording on your phone) so that it is easier to reproduce later for live classification.\n",
    "\n",
    "Please read the full instructions for these tasks before proceeding.\n",
    "**For each chosen word, do the following:**\n",
    "1. Run the cell below. Add the file name\n",
    "    - The cell will ask for a filename to your csv file recording. The purpose of this csv file is to preserve your recordings as a permanent file, rather than as a temporary variable in jupyter notebook. This allows you to reuse the recordings you made in case you decide to take a break between the lab, or accidentally restart your jupyter notebook. \n",
    "    - We recommend naming your csv file to: `YOUR_WORD.csv`\n",
    "2. Begin Recording. Each time you press enter, say your word once within the 3 second interval.\n",
    "    - **Pronounce the word consistently**\n",
    "    - \"Good\" audio data has a high signal to noise ratio (SNR). Recording words while far away from the microphone may cause your intended word to blend in with background noise. However, speaking too loudly and/or too closely into the mic) may also cause your output to rail. Here's what a railing (clipping) audio signal may look like:\n",
    "    ![clippingsignal](clippingsignal.png)\n",
    "    - You can see that this signal is utilizing almost the maximum magnitude of signal allowed in this audio file format for the majority of the duration. When an audio signal is too loud, its magnitude will exceed the maximum allowed value and \"rail\" to that value. We don't want this--please re-record further away from your mic if this happens.\n",
    "    - **We recommend that after taking 3 or 4 recordings for the first time, stop the program by entering \"stop\" in the text box popup and check `YOUR_WORD.csv` make sure that it looks like a sound wave as opposed to being full of super low values. *It might help to graph the data as a line plot in Excel.*** You don't want to record 40 times only to find that your recordings weren't working.\n",
    "    - Type \"d\" to delete the most recent recording\n",
    "    - repeat to make 40 recording of each word.\n",
    "\n",
    "### Before moving on, please note that:\n",
    "\n",
    "You may realize in the next section that one or two of your words are not sorting quite as well as you would like. Don't be afraid to come back to this section and try collecting different words based on what you have learned makes a word sortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the recording script\n",
    "utils.create_recording_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='part2'></a>\n",
    "## <span style=\"color:navy\">Task 2: Data Preprocessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different recordings of the same word can look wildly different, depending on factors like when you started saying the word and how quickly you said it (assuming you are not a robot that can repeat the word 40 times exactly the same way). Thus, before we can use the recorded data for PCA, we must first process the data. We will do this according to the following procedure:\n",
    "1. Split the dataset into a training dataset and a test dataset\n",
    "2. Align the audio recordings\n",
    "3. Apply an envelope (low pass filter) to the recordings to remove irrelevant high-frequency components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: fill in the blank for the word you just recorded\n",
    "all_words_arr = ['jack', 'jason', 'jessica', 'entanglement', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a: Align Audio Recordings\n",
    "Let's begin by splitting our data into a training and testing set with a 70/30 split. Run the code below to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data from csv\n",
    "train_test_split_ratio = 0.7\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "# Build the dictionary of train and test samples.\n",
    "for i in range(len(all_words_arr)):\n",
    "    word_raw = utils.read_csv(\"{}.csv\".format(all_words_arr[i]))\n",
    "    word_raw_train, word_raw_test = utils.train_test_split(word_raw, train_test_split_ratio)\n",
    "    train_dict[all_words_arr[i]] = word_raw_train\n",
    "    test_dict[all_words_arr[i]] = word_raw_test\n",
    "\n",
    "# Count the minimum number of samples you took across the six recorded words. These variables might be useful for you!\n",
    "num_samples_train = min(list(map(lambda x : np.shape(x)[0], train_dict.values())))\n",
    "num_samples_test = min(list(map(lambda x : np.shape(x)[0], test_dict.values())))\n",
    "\n",
    "# Crop the number of samples for each word to the minimum number so all words have the same number of samples.\n",
    "for key, raw_word in train_dict.items():\n",
    "    train_dict[key] = raw_word[:num_samples_train,:]\n",
    "\n",
    "for key, raw_word in test_dict.items():\n",
    "    test_dict[key] = raw_word[:num_samples_test,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at all the training samples.\n",
    "\n",
    "**<span style=\"color:red\">Important: It's okay if the recordings aren't aligned. The code in the next part will align the data.</span>** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot all training samples\n",
    "word_number = 0\n",
    "selected_words_arr = all_words_arr\n",
    "for word_raw_train in train_dict.values():\n",
    "    plt.plot(word_raw_train.T)\n",
    "    plt.title('Training sample for \"{}\"'.format(selected_words_arr[word_number]))\n",
    "    word_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b: Align Audio Recordings\n",
    "\n",
    "As seen above, the speech is a fraction of the 3 second window, and each sample starts at different times. PCA is not good at interpreting delay, so we need to align the recordings and trim to a smaller segment of the sample where the speech is present. To do this, we will use a thresholding algorithm.\n",
    "\n",
    "First, we define a **`threshold`** relative to the maximum value of the data. We say that any signal that crosses the threshold is the start of a speech command. In order to not lose the first couple samples of the speech command, we say that the command starts **`pre_length`** samples _before_ the threshold is crossed. We then use a window of the data that is **`length`** long, and try to capture the entire command in that window.\n",
    "\n",
    "**Play around with the parameters `length`, `pre_length` and `threshold`** in the cells below to find appropriate values corresponding to your voice and chosen commands. You should see the results and how much of your command you captured in the plots generated below.\n",
    "\n",
    "We also pass in a `envelope=True` argument to our `process_data` function: this filters out the frequencies higher than our vocal range, to get back an \"envelope\" of the waveform of our voice recordings. Why might this be useful?\n",
    "\n",
    "Also note that we're \"normalizing\" the signal towards the end--this is very common in a lot of signal processing applications; another name for this is \"min-max feature scaling,\" where rescale the amplitude range of the signal to be in $[0,1]$ by dividing the signal by the maximum absolute value. This is different from taking a norm of a vector as we've seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_recording(recording, length, pre_length, threshold, envelope=False):\n",
    "    \"\"\"\n",
    "    align a single audio samples according to the given parameters.\n",
    "    \n",
    "    Args:\n",
    "        recording (np.ndarray): a single audio sample.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "        envelope (bool): if True, use enveloping.\n",
    "    \n",
    "    Returns:\n",
    "        aligned recording.\n",
    "    \"\"\"\n",
    "    \n",
    "    if envelope:\n",
    "        recording = utils.envelope(recording, 5400, 100)\n",
    "    \n",
    "    # Find the threshold\n",
    "    recording_threshold = threshold * np.max(recording)\n",
    "\n",
    "    # TODO: Use recording_threshold, length, and prelength to cut the snippet to the correct length\n",
    "    # we note where the recording magnitude first exceeds recording_threshold.\n",
    "    # then, we leave prelength number of samples before the crossing of the threshold, which is where our recording starts.\n",
    "    # we cut the recording so that it's 'length' samples away from the start of the recording.\n",
    "    \n",
    "    i = pre_length\n",
    "    while ... : # YOUR CODE HERE\n",
    "        i += 1\n",
    "\n",
    "    snippet_start = min(i - pre_length, len(recording) - length) # YOUR CODE HERE\n",
    "    snippet = recording[...] # YOUR CODE HERE\n",
    "\n",
    "    # TODO: Normalize the recording.\n",
    "    # \"Normalize\" in our case is dividing the signal by the maximum absolute value (different from taking the norm of a vector)\n",
    "    snippet_normalized = ... # YOUR CODE HERE\n",
    "    \n",
    "    return snippet_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_data(data, length, pre_length, threshold, envelope=False):\n",
    "    \"\"\"\n",
    "    align all audio samples in dataset. (apply align_recording to all rows of the data matrix)\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Matrix where each row corresponds to a recording's audio samples.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "    \n",
    "    Returns:\n",
    "        Matrix of aligned recordings.\n",
    "    \"\"\"\n",
    "    assert isinstance(data, np.ndarray) and len(data.shape) == 2, \"'data' must be a 2D matrix\"\n",
    "    assert isinstance(length, int) and length > 0, \"'length' of snippet must be an integer greater than 0\"\n",
    "    assert 0 <= threshold <= 1, \"'threshold' must be between 0 and 1\"\n",
    "    snippets = []\n",
    "\n",
    "    # Iterate over the rows in data\n",
    "    for recording in data:\n",
    "        snippets.append(align_recording(recording, length, pre_length, threshold, envelope))\n",
    "\n",
    "    return np.vstack(snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(dict_raw, length, pre_length, threshold, plot=True, envelope=False):\n",
    "    \"\"\"\n",
    "    Process the raw data given parameters and return it. (wrapper function for align_data)\n",
    "    \n",
    "    Args:\n",
    "        dict_raw (np.ndarray): dictionary of all words: data matrix.\n",
    "        length (int): The length of each aligned audio snippet.\n",
    "        pre_length (int): The number of samples to include before the threshold is first crossed.\n",
    "        threshold (float): Used to find the start of the speech command. The speech command begins where the\n",
    "            magnitude of the audio sample is greater than (threshold * max(samples)).\n",
    "        plot (boolean): Plot the dataset if true.\n",
    "            \n",
    "    Returns:\n",
    "        Processed data dictionary.\n",
    "    \"\"\"\n",
    "    processed_dict = {}\n",
    "    word_number = 0\n",
    "    for key, word_raw in dict_raw.items():\n",
    "        word_processed = align_data(word_raw, length, pre_length, threshold, envelope=envelope)\n",
    "        processed_dict[key] = word_processed\n",
    "        if plot:\n",
    "            plt.plot(word_processed.T)\n",
    "            plt.title('Samples for \"{}\"'.format(selected_words_arr[word_number]))\n",
    "            word_number += 1\n",
    "            plt.show()\n",
    "            \n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Edit the parameters to get the best alignment.\n",
    "length = ... # YOUR CODE HERE\n",
    "pre_length = 400 # Modify this as necessary\n",
    "threshold = ... # YOUR CODE HERE\n",
    "\n",
    "processed_train_dict = process_data(train_dict, length, pre_length, threshold, envelope=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now notice the improved alignment for the samples. Can you tell which word is which just by the envelope? If any of your words look too similar to each another, then PCA will likely have a difficult time distinguishing between them, but the STFT version of PCA might circumvent this issue. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task3'></a>\n",
    "## <span style=\"color:navy\">Task 3: PCA via SVD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD/PCA Resources\n",
    "- http://www.ams.org/publicoutreach/feature-column/fcarc-svd\n",
    "- https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
    "- https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: Generate and Preprocess PCA Matrix\n",
    "\n",
    "Now that we have our aligned data, we will build the PCA input matrix from that data by stacking all the data vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_A = np.vstack(list(processed_train_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of PCA is to center the data's mean at zero and store it in demeaned_A. Please note that you want to get the mean of each feature (what are the features?). The function np.mean might be helpful here, along with specifying the axis parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zero-mean the matrix A\n",
    "mean_vec = ... # YOUR CODE HERE\n",
    "demeaned_A = ... # YOUR CODE HERE\n",
    "print(processed_A.shape)\n",
    "print(mean_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: PCA\n",
    "\n",
    "Take the SVD of your demeaned data - `np.linalg.svd` may be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take the SVD of matrix demeaned_A\n",
    "U, S, Vt = ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect your sigma values. They should tell you how many principal components you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot out the sigma values (Hint: Use plt.stem for a stem plot)\n",
    "... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3c: Choosing a Basis using Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `new_basis` argument to be a basis of the first 3 principal components.\n",
    "\n",
    "Hint 1: Of the three outputs from the SVD function call, which one will contain the principal components onto which we want to project our data points? Do we need to transpose it?\n",
    "\n",
    "Hint 2: For $A \\in \\mathbb{R}^{n \\times n}$, $U$ contains the basis of the column space of $A$, and $V$ contains the basis of the row space of $A$.\n",
    "\n",
    "Sanity check: When you plot `new_basis` you should see a number of line plots equal to the number of principal components you've chosen. You should have 3 differently colored components, each of length ‘length’ that you chose above.\n",
    "\n",
    "**NOTE: The projection onto a subspace with orthogonal basis vectors (i.e. a matrix with orthogonal columns) reduces to an inner product:**\n",
    "\n",
    "$$\\text{proj}_{\\text{Col(Q)}} (\\vec{y}) = QQ^T \\vec{y} = \\sum^n_{i=1} \\langle \\vec{y}, \\vec{q}_i \\rangle \\vec{q}_i$$\n",
    "\n",
    "refer to eecs16b's [Spring 2024 note 13](https://eecs16b.org/notes/sp24/note13.pdf) for a proof of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the principal component(s)\n",
    "new_basis = ... # YOUR CODE HERE\n",
    "plt.plot(new_basis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now project the data in the matrix A onto the new basis and plot it. For three principal components, in addition to the 3D plot, we also provided 2D plots which correspond to the top and side views of the 3D plot. Do you see clustering? Do you think you can separate the data easily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project the data onto the new basis\n",
    "# Hint: np.dot() may help, as well as printing the dimensions.\n",
    "proj = np.dot(demeaned_A, new_basis) # YOUR CODE HERE\n",
    "\n",
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(all_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_train:num_samples_train*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_train:num_samples_train*(i+1),1], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], edgecolor='none')\n",
    "\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in many AI applications, the data above are noisy, so we expect some classification errors. The important part is that you see strong clustering of your words. \n",
    "\n",
    "If you don't see clustering, try to think about why this might be the case. Things you might want to ask yourself:\n",
    "- How does PCA create the clusters? \n",
    "- Which characteristics of your waveform will PCA favor when clustering? \n",
    "- How can you choose your words to maximize the differences between the classes?\n",
    "\n",
    "Once you think you have decent clustering, move on to automating classification. **Choose 4 out of the 6 words which form the most distinct clusters. You will be using these four words for the rest of this lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task4'></a>\n",
    "## <span style=\"color:navy\">Task 4: Clustering Data Points</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement `find_centroids`, which finds the center of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_centroid(clustered_data):\n",
    "    \"\"\"Find the center of each cluster by taking the mean of all points in a cluster.\n",
    "    It may be helpful to recall how you constructed the data matrix (e.g. which rows correspond to which word)\n",
    "    \n",
    "    Parameters:\n",
    "        clustered_data (np.array): the data already projected onto the new basis (for one word)\n",
    "        \n",
    "    Returns: \n",
    "        centroids (list): The centroids of the clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    return ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run find_centroid on each word\n",
    "centroids = []\n",
    "for i in range(len(all_words_arr)):\n",
    "    centroid = find_centroid(proj[i*num_samples_train:(i + 1)* num_samples_train])\n",
    "    centroids.append(centroid)\n",
    "\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to plot your centroids along with your projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "centroid_list = np.vstack(centroids)\n",
    "colors = cm[:(len(centroids))]\n",
    "\n",
    "for i, centroid in enumerate(centroid_list):\n",
    "    print('Centroid {} is at: {}'.format(i, str(centroid)))\n",
    "\n",
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_train:num_samples_train*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(selected_words_arr, loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    for i in range(len(selected_words_arr)):\n",
    "        Axes3D.scatter(ax, *np.array([centroids[i]]).T, c=cm[i], marker = '*', s=300)\n",
    "    plt.title(\"Training Data\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_train:num_samples_train*(i+1),1], proj[i*num_samples_train:num_samples_train*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(all_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axs[0].scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    axs[1].scatter(centroid_list[:,0], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    axs[2].scatter(centroid_list[:,1], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    plt.show()\n",
    "\n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_train:num_samples_train*(i+1),0], proj[i*num_samples_train:num_samples_train*(i+1),1], c=colors[i], edgecolor='none')\n",
    "\n",
    "    plt.scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(\"Training Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task5'></a>\n",
    "## <span style=\"color:navy\">Task 5: Testing your Classifier</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have the means (centroid) for each word, let's evaluate performance. Recall that we will classify each data point according to the centroid with the least Euclidian distance to it.\n",
    "\n",
    "Before we perform classification, we need to do the same preprocessing to the test data that we did to the training data (enveloping, demeaning, projecting onto the PCA basis). You have already written most of the code for this part. However, note the difference in variable names as we are now working with test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at what our raw test data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5a: Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat what we did for our training data, so we can try out the test data on our trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot all test samples\n",
    "word_number = 0\n",
    "for word_raw_test in test_dict.values():\n",
    "    plt.plot(word_raw_test.T)\n",
    "    plt.title('Test sample for \"{}\"'.format(all_words_arr[word_number]))\n",
    "    word_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform enveloping and trimming of our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_test_dict = process_data(test_dict, length, pre_length, threshold, plot=True, envelope=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the PCA matrix. Refer to the code we used above for the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_A_test = ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will do something slightly different.**\n",
    "\n",
    "Previously, you projected data onto your PCA basis with $ (x - \\bar{x})P $, where $\\bar{x}$ is the mean vector, $x$ is a single row of `processed_A`, and $P$ is `new_basis`. \n",
    "\n",
    "We can rewrite this operation as:\n",
    "\n",
    "$$ (x - \\bar{x})P = xP - \\bar{x}P = xP - \\bar{x}_{\\text{proj}} $$ \n",
    "$$ \\bar{x}_{\\text{proj}} = \\bar{x}P $$\n",
    "\n",
    "Why might we want to do this? In the real world, we want our data to take up as little storage as possible. Instead of storing a length $n$ vector $\\bar{x}$, we can precompute $ \\bar{x}_{\\text{proj}} \\in \\mathbb{R}^3$ and store that instead!\n",
    "\n",
    "Compute $ \\bar{x}_{\\text{proj}} $ using the **same mean vector** as the one computed with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "projected_mean_vec = ... # YOUR CODE HERE\n",
    "print(new_basis.shape)\n",
    "print(mean_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the test data onto the **same PCA basis** as the one computed with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "projected_A_test = ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-mean the projected test data using the **`projected_mean_vec`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proj = ... # YOUR CODE HERE\n",
    "print(projected_mean_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the projections to see how well your test data clusters in this new basis. This will give you an idea of test classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if new_basis.shape[1] == 3:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(all_words_arr)):\n",
    "        Axes3D.scatter(ax, *proj[i*num_samples_test:num_samples_test*(i+1)].T, c=cm[i], marker = 'o', s=20)\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1.07, 0.5))\n",
    "    plt.title(\"Test Data\")\n",
    "    for i in range(len(all_words_arr)):\n",
    "        Axes3D.scatter(ax, *np.array([centroids[i]]).T, c=cm[i], marker = '*', s=300)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15,5))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        axs[0].scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),1], c=cm[i], edgecolor='none')\n",
    "        axs[1].scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),2], c=cm[i], edgecolor='none')\n",
    "        axs[2].scatter(proj[i*num_samples_test:num_samples_test*(i+1),1], proj[i*num_samples_test:num_samples_test*(i+1),2], c=cm[i], edgecolor='none')\n",
    "    axs[0].set_title(\"View 1\")\n",
    "    axs[1].set_title(\"View 2\")\n",
    "    axs[2].set_title(\"View 3\")\n",
    "    plt.legend(all_words_arr, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    axs[0].scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    axs[1].scatter(centroid_list[:,0], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    axs[2].scatter(centroid_list[:,1], centroid_list[:,2], c=colors, marker='*', s=300)\n",
    "    fig.show()\n",
    "\n",
    "elif new_basis.shape[1] == 2:\n",
    "    fig=plt.figure(figsize=(10,7))\n",
    "    for i in range(len(all_words_arr)):\n",
    "        plt.scatter(proj[i*num_samples_test:num_samples_test*(i+1),0], proj[i*num_samples_test:num_samples_test*(i+1),1], c=colors[i], edgecolor='none')\n",
    "\n",
    "    plt.scatter(centroid_list[:,0], centroid_list[:,1], c=colors, marker='*', s=300)\n",
    "    plt.legend(all_words_arr,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(\"Test Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some idea of how our test data looks in our PCA basis, let's see how our data actually performs. Implement the `classify` function which takes in a data point after enveloping is applied and returns which word number it belongs to depending on the closed centroid in Euclidian distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify(word_set, data_point, new_basis, projected_mean_vec, centroids):\n",
    "    \"\"\"Classifies a new voice recording into a word.\n",
    "    \n",
    "    Args:\n",
    "        word_set (list): set of words that we've chosen\n",
    "        data_point (np.array): new data point vector before demeaning and projection\n",
    "        new_basis (np.array): the new processed basis to project on\n",
    "        projected_mean_vec (np.array): the same projected_mean_vec as before\n",
    "    Returns:\n",
    "        (string): The classified word\n",
    "    Hint:\n",
    "        Remember to use 'projected_mean_vec'!\n",
    "        np.argmin(), and np.linalg.norm() may also help!\n",
    "    \"\"\"\n",
    "    # TODO: classify the demeaned data point by comparing its distance to the centroids\n",
    "    # YOUR CODE HERE\n",
    "    projected_data_point = ... # YOUR CODE HERE\n",
    "    \n",
    "    # hint: demean with the precomputed mean vector\n",
    "    demeaned = ... # YOUR CODE HERE \n",
    "    \n",
    "    # hint: we're returning the word that this data point classified as\n",
    "    return ... # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the classification function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try out the classification function\n",
    "print(classify(all_words_arr, processed_A_test[0,:], new_basis, projected_mean_vec, centroids)) # Modify the row index of processed_A_test to use other vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our goal is 80% accuracy for each word.** Apply the `classify` function to each sample and compute the accuracy for each word. If you do not meet 80% accuracy for each word, try to find different combinations of words/parameters that result in more distinct clusters in the plots of the projected data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_A_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try to classify the whole A matrix\n",
    "correct_counts = np.zeros(len(all_words_arr))\n",
    "\n",
    "for (row_num, data) in enumerate(processed_A_test):\n",
    "    word_num = row_num // num_samples_test\n",
    "    if classify(all_words_arr, data, new_basis, projected_mean_vec, centroids) == all_words_arr[word_num]:\n",
    "        correct_counts[word_num - 1] += 1\n",
    "        \n",
    "for i in range(len(correct_counts)):\n",
    "    print(\"Percent correct of word {} = {}%\".format(all_words_arr[i], 100 * correct_counts[i] / num_samples_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='task6'></a>\n",
    "## <span style=\"color:navy\">Task 6: Testing the classifier -- Real Time</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll be testing the classifier in real time. Run the script below, and when prompted, say one of your chosen words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rate = 5400  # Sample rate\n",
    "chunk = 1024  # Chunk size\n",
    "record_seconds = 3  # Record duration in seconds\n",
    "num_recordings = 50  # Total number of recordings needed\n",
    "recording_count = 0\n",
    "word = \"\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Press Enter to start recording, or type 'stop' and then Enter to stop recording: \")\n",
    "\n",
    "    if user_input == '':\n",
    "        # Record audio\n",
    "        audio_recording = utils.record_audio(seconds=record_seconds, rate=rate, chunk=chunk)\n",
    "\n",
    "        # Preprocess the single data. Hint: align_recording() might be useful here! make sure to set envelope=True.\n",
    "        processed_audio_recording = ... # YOUR CODE HERE\n",
    "        \n",
    "        # Classify\n",
    "        print(\"Classified Word: \" + ...) # YOUR CODE HERE\n",
    "        time.sleep(2)\n",
    "       \n",
    "    if user_input == 'stop':\n",
    "        display.clear_output()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">CHECKOFF</span>\n",
    "\n",
    "### When you are ready to get checked off, fill out the **[Checkoff Google Form](https://docs.google.com/forms/d/e/1FAIpQLSfIOjvEJXew-M0-h9uJ3C25UOdmmABFK0GGNl3o9p7po7Cc0A/viewform?usp=sf_link)**\n",
    "\n",
    "\n",
    "- **Have all questions, code, and plots completed in this notebook.** Your TA will check all your PCA code and plots.\n",
    "- **Show your GSI that you've achieved 80% accuracy on your test data for all 4 words.** \n",
    "- **Show your GSI that you are able to classify live**\n",
    "- **Be prepared to answer conceptual questions about the lab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
